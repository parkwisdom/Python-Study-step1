1.텍스트 분류
-스팸/햄 메일
-가장 많이 사용되는 방법: 베이즈 정리를 이용한 텍스트 분류 방법인 '베이지안 필터'

2.머신러닝
-교사,비교사,강화 학습
-베이지안 필터는 교사 학습에 해당

3.모델생성 과정
스팸/햄 메일 입력 -> 메일내용학습 -> 모델 -> 새로운 메일 입력 --> 스팸/햄 분류 결과 출력

4.조건부 확률
- 어떤 A라는 사건이 일어났다는 조건하에서 다른 사건 B가 일어날 확률 P(B|A)
- 비가 내릴 확률 :P(비),교통사고가 발생할 확률 :P(교통사고)
- 비가 내리는 날에 교통사고가 발생할 확률? P(교통사고 | 비)
- P(B|A) = P(A교B)/P(A) (단,P(A)가 0이 아님)
- P(A교B) =P(B|A)*P(A)
- ex)숫자(1~20)카드 ,A:2의 배수가 나오는 사건, B:3의 배수가 나오는 사건
P(B|A) =P(A교B)/P(A) = (3/20) /(10/20)=3/10(2의 배수의 개수, 표본 공간 변화) => 확률 30프로
     안경  노안경
남자  5       7
여자  6       4
A: 남자일 사건, B:안경을 쓰고 있을 사건
P(B|A)=P(A교B)/P(A) = (5/22)/(12/22)=5/12

-두사건이 서로 독립인 경우: P(A교B)=P(A)*P(B),P(A|B)=P(A),P(B|A)=P(B)
-두사건이 서로 종속인 경우: P(A교B)=P(B|A)*P(A),P(B|A)=P(A교B)/P(B)

5.베이즈 정리
P(B|A)
P(A):A가 일어날 확률
P(B):B가 일어날 확률
P(A|B):B가 일어난 후에 A가 일어날 확률(사후 확률)
P(B|A):A가 일어난 후에 B가 일어날 확률 (사전 확률)
-상정의 매출액을
가장 잘 파리는 상품:노트와 사인펜
-'전체 손님','노트를 구매한 손님','사이펜을 구매한 손님','두개를 모두 구매한 손님'
전체:100
노트:50
사이펜:20
두가지모두:10
--------------
다음 손님이 구입할 물건이 뭔지 예측?
A:노트 구매 사건
B:사인펜 구매 사건
노트와 사인펜을 동시에 구매할 확률 => 50/100 * 10/50=500/5000 =1/10

조건부 확률P(A|BG):P(B)사인펜을 구입한다는 조건하에서 P(A)노트를 구입할 확률
B와 A의 결합확률 : P(A|B)*P(B)

p(A|B)=P(A교B)/P(B)
P(A교B)=p(A|B)*P(B)
P(B교A)=P(B|A)*P(A)
P(B교A)=P(A교B)
P(A교B)=P(B|A)*P(A)=P(A|B)*P(B)
P(B|A)=(P(A|B)*P(B))/P(A)

결론:B와 A의 결합확률=A와 B의 결합확률(P(B|A)*P(A)=P(A|B)*P(B))

6.나이브 베이즈 분류기
-베이지안 필터는 나이브 베이즈 분류 알교리즘을 사용한 것이다.
-나이브 베이즈 분류는 베이즈 정리를 사용한 분류 방법
-베이즈 정리: A라는 사건이 B에 속하는지를 판단할 떄 사용.
    A가 입력 텍스트,B가 카테고리 판정 결과(B1,B2,B3/...Bn;스팸,햄)
-어떤 문장을 카테고리 분류할 때, 나이브 베이즈 분류는 텍스트 내부에서 단어의 출현 비율을 조사 =>기반으로 텍스트를 어떤 카테고리로 분류하는게 좋은지 판단
-P(B|A) : 베이즈 정리에서 분모에 있는 P(A)는 입력 텍스트가 주어질 확률.
    어떤 카테고리를 판정하든 같은 입력텍스트가 주어지는 것이므로(같은 값으로 생각할 수 있음)따로 교려하지 않아도 됨
    P(B|A)=P(B)*P(A|B)
    p(B): 각 카테고리로 분류될 확률(전체 문서에서 해당 카테고리의 문서일 확률)
    P(A|B): A는 입력 텍스트의 단어의 집합, 단어들의 순서는 고려하지 않고, 단어들만 표현한다(Bag-Of-Word; BOW)

    입력 텍스트: A를 각 단어(aN)의 집합이라고 할 때 , P(A|B)?
    P(A|B)=P(a1|B)P(a2|B).....p(aN|B)
    P(aN|B)?단어가 카테고리에 속할 확률
    어떤 카테고리에 해당 단어가 출현한 확률을 계산. 출현율=출현 횟수/카테고리 전체 단어 수

